{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Data and embed \n",
    "\n",
    "    This notebook is used as a playground\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import statistics\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For embeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "## Loaders\n",
    "from langchain.document_loaders import DataFrameLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_directory = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['book_number', 'book_name', 'chapter_name', 'title', 'commentary',\n",
       "       'text', 'section_number', 'chunk_id', 'source', 'num_tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_data = pd.read_csv(out_directory/'kaggle_tilak_summaries.csv', sep=\"|\")\n",
    "kaggle_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'section_number', 'title', 'chapter_number', 'chapter_name',\n",
       "       'chunk_id', 'source', 'num_tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinytales_data = pd.read_csv(out_directory/'tiny_tales_summaries.csv', sep=\"|\")\n",
    "tinytales_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['book', 'source', 'title', 'book_number', 'description', 'text',\n",
       "       'chunk_id', 'num_tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_data = pd.read_csv(out_directory/'wikipedia_parva_summaries.csv', sep=\"|\")\n",
    "\n",
    "## Droping unnecessary columns\n",
    "wikipedia_data.drop(['start_chapter', 'end_chapter'], axis=1, inplace=True)\n",
    "wikipedia_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the dataframes into one big dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle data dims (2376, 10) \n",
      " TinyTales data dims (200, 8) \n",
      " Wikipedia data dims (19, 8) \n",
      " Final data dims (2595, 13)\n",
      "Final data columns \n",
      " Index(['book_number', 'book_name', 'chapter_name', 'title', 'commentary',\n",
      "       'text', 'section_number', 'chunk_id', 'source', 'num_tokens',\n",
      "       'chapter_number', 'book', 'description'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_combined = pd.concat([kaggle_data, tinytales_data, wikipedia_data])\n",
    "print(\n",
    "    \"Kaggle data dims\",  kaggle_data.shape, \"\\n\",\n",
    "    \"TinyTales data dims\", tinytales_data.shape, \"\\n\",\n",
    "    \"Wikipedia data dims\", wikipedia_data.shape, \"\\n\",\n",
    "    \"Final data dims\", df_combined.shape)\n",
    "\n",
    "print(\"Final data columns \\n\", df_combined.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the final dataframe into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_number       float64\n",
      "book_name          object\n",
      "chapter_name       object\n",
      "title              object\n",
      "commentary         object\n",
      "text               object\n",
      "section_number    float64\n",
      "chunk_id           object\n",
      "source             object\n",
      "num_tokens          int64\n",
      "chapter_number    float64\n",
      "book               object\n",
      "description        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_combined.to_csv(out_directory/'summaries_combined.csv', index=False, sep=\"|\")\n",
    "print(df_combined.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedd and persist into PG Vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_model = \"text-embedding-ada-002\"\n",
    "\n",
    "CONNECTION_STRING = PGVector.connection_string_from_db_params(\n",
    "    driver=\"psycopg2\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\",\n",
    "    database=os.environ[\"PGVECTOR_DATABASE\"],\n",
    "    user=os.environ[\"PGVECTOR_USER\"],\n",
    "    password=os.environ[\"PGVECTOR_PASSWORD\"],\n",
    ")\n",
    "\n",
    "COLLECTION_NAME = \"mh_embeddings_summaries\"\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=text_embedding_model)\n",
    "\n",
    "store = PGVector(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    embedding_function=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataframe into a loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(df_combined, page_content_column=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Helper funciton to combine tokens into names\n",
    "def combine_tokens(ner_results):\n",
    "    name = \"\"\n",
    "    entities = []\n",
    "    for res in ner_results:\n",
    "        word = res['word']\n",
    "        if word[0] == \"▁\":\n",
    "            if not name == \"\":\n",
    "                entities = entities + [{'name': name, 'entity': entity}]\n",
    "            name = word[1:]\n",
    "            entity = res['entity']\n",
    "        elif not word in [',', \"'\", \".\", 's', \"'\", \";\", \"(\", \")\"]:\n",
    "            name = name + word\n",
    "    \n",
    "    ## append the last name\n",
    "    entities = entities + [{'name': name, 'entity': entity}]\n",
    "    ## Return\n",
    "    return entities\n",
    "\n",
    "## Get names entities\n",
    "def recognise_named_entities(text, pipeline_model):\n",
    "    ner_results = pipeline_model(text)\n",
    "    return ner_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text \u001b[39m=\u001b[39m docs[\u001b[39m100\u001b[39m]\u001b[39m.\u001b[39mpage_content\n\u001b[1;32m      2\u001b[0m \u001b[39m# print(text)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "text = docs[100].page_content\n",
    "# print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta Named Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277456901"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Roberta based NER\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"2rtl3/mn-xlm-roberta-base-named-entity\")\n",
    "roberta_model = AutoModelForTokenClassification.from_pretrained(\"2rtl3/mn-xlm-roberta-base-named-entity\")\n",
    "nlp_roberta = pipeline(\"ner\", model=roberta_model, tokenizer=roberta_tokenizer)\n",
    "roberta_model.num_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_results = recognise_named_entities(text, nlp_roberta)\n",
    "entities = combine_tokens(ner_results)\n",
    "df_roberta = pd.DataFrame(entities)\n",
    "\n",
    "# print(text)\n",
    "# for entity in entities:\n",
    "#     print(entity)\n",
    "# for res in ner_results:\n",
    "#     print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbert NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11099913"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arbert_tokenizer = AutoTokenizer.from_pretrained(\"ArBert/albert-base-v2-finetuned-ner\")\n",
    "arbert_model = AutoModelForTokenClassification.from_pretrained(\"ArBert/albert-base-v2-finetuned-ner\")\n",
    "nlp_arbert = pipeline(\"token-classification\", model=arbert_model, tokenizer=arbert_tokenizer)\n",
    "arbert_model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_results = recognise_named_entities(text, nlp_arbert)\n",
    "entities = combine_tokens(ner_results)\n",
    "df_arbert = pd.DataFrame(entities)\n",
    "df_arbert = df_arbert.loc[df_arbert['entity'] != 'LABEL_0']\n",
    "\n",
    "# print(text)\n",
    "# for res in ner_results:\n",
    "#     print(res)\n",
    "# for entity in entities:\n",
    "#     print(entity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndicBert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
    "# model = AutoModel.from_pretrained('ai4bharat/indic-bert')\n",
    "\n",
    "# inputs = tokenizer(\"After Abhimanyu's marriage, there was royal festival and everyone was pleased\", return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# outputs.pooler_output.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAI@3114",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
